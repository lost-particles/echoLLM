{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4esdo5XCG7b"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium langchain chromadb --quiet\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "\n",
        "# LangChain imports for memory\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import CharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Log in to Hugging Face Hub\n",
        "login_token = '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'\n",
        "login(login_token)\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# model options:\n",
        "# model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "# model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "# model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU available. Using device:\", device)"
      ],
      "metadata": {
        "id": "UrjZ3r9fCbwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "\n",
        "# Setup LangChain Memory\n",
        "# 1. Vector store for storing past experiences\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = Chroma(embedding_function=embeddings, persist_directory=\"./chroma_db\")\n",
        "\n",
        "# 2. Conversation buffer for recent interactions\n",
        "conversation_memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "class LLMMemory:\n",
        "    def __init__(self, vector_store, conversation_memory, max_conversation_length=10):\n",
        "        self.vector_store = vector_store\n",
        "        self.conversation_memory = conversation_memory\n",
        "        self.max_conversation_length = max_conversation_length\n",
        "        self.experience_count = 0\n",
        "        self.text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "\n",
        "    def add_experience(self, state, action, next_state, reward, grid_map):\n",
        "        \"\"\"Add a new experience to memory\"\"\"\n",
        "        action_map = ['left', 'down', 'right', 'up']\n",
        "        action_name = action_map[action]\n",
        "\n",
        "        experience = (\n",
        "            f\"Experience #{self.experience_count}: \"\n",
        "            f\"When in state {state}, taking action {action_name} \"\n",
        "            f\"led to state {next_state} with reward {reward:.2f}.\"\n",
        "        )\n",
        "\n",
        "        # Add to conversation memory\n",
        "        self.conversation_memory.chat_memory.add_user_message(experience)\n",
        "\n",
        "        # Periodically move older experiences to vector store\n",
        "        if len(self.conversation_memory.chat_memory.messages) > self.max_conversation_length:\n",
        "            old_messages = self.conversation_memory.chat_memory.messages[:-self.max_conversation_length]\n",
        "            self.conversation_memory.chat_memory.messages = self.conversation_memory.chat_memory.messages[-self.max_conversation_length:]\n",
        "\n",
        "            # Add old messages to vector store\n",
        "            for msg in old_messages:\n",
        "                docs = self.text_splitter.create_documents([msg.content])\n",
        "                self.vector_store.add_documents(docs)\n",
        "\n",
        "        # Also add current experience to vector store\n",
        "        docs = self.text_splitter.create_documents([experience])\n",
        "        self.vector_store.add_documents(docs)\n",
        "\n",
        "        self.experience_count += 1\n",
        "\n",
        "    def retrieve_similar_experiences(self, state, action, top_k=3):\n",
        "        \"\"\"Retrieve similar past experiences\"\"\"\n",
        "        action_map = ['left', 'down', 'right', 'up']\n",
        "        action_name = action_map[action]\n",
        "\n",
        "        query = f\"state {state} action {action_name}\"\n",
        "        docs = self.vector_store.similarity_search(query, k=top_k)\n",
        "        return [doc.page_content for doc in docs]\n",
        "\n",
        "    def get_recent_history(self):\n",
        "        \"\"\"Get recent conversation history\"\"\"\n",
        "        return self.conversation_memory.chat_memory.messages\n",
        "\n",
        "# Initialize memory\n",
        "llm_memory = LLMMemory(vectorstore, conversation_memory)"
      ],
      "metadata": {
        "id": "1Z15b0NzCxuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_language_reward(state, action, next_state, grid_map, memory=None):\n",
        "    action_map = ['left', 'down', 'right', 'up']\n",
        "    action_name = action_map[action]\n",
        "\n",
        "    # Get similar experiences if memory is available\n",
        "    similar_experiences = \"\"\n",
        "    if memory:\n",
        "        experiences = memory.retrieve_similar_experiences(state, action, top_k=3)\n",
        "        similar_experiences = \"Past similar experiences:\\n\" + \"\\n\".join(experiences) if experiences else \"\"\n",
        "\n",
        "    prompt = (\n",
        "        f\"### Instruction:\\n\"\n",
        "        f\"You are evaluating a move made by an agent in the Frozen Lake game.\\n\"\n",
        "        f\"The lake is a 4x4 grid with 16 states (0 to 15), where the agent starts at state 0 and must reach the goal at state 15.\\n\"\n",
        "        f\"There are holes that will end the game if the agent falls in, and loops or unnecessary steps should be avoided.\\n\\n\"\n",
        "        f\"The key is: S=Start, F=Frozen, H=Hole, G=Goal.\\n\"\n",
        "        f\"The layout of the grid is: {grid_map}.\\n\"\n",
        "        f\"The agent moved from state {state} to state {next_state} by going {action_name}.\\n\"\n",
        "        f\"How good was this move on a scale from 0 (very bad) to 1 (excellent)?\\n\"\n",
        "        f\"Respond with a single decimal number only.\\n\"\n",
        "        f\"### Response:\\n\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    inputs = inputs.to(device)\n",
        "\n",
        "    outputs = model.generate(**inputs, max_new_tokens=10)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    try:\n",
        "        reward_str = response.split(\"### Response:\")[-1].strip()\n",
        "        reward_val = float(reward_str.split()[0])\n",
        "        reward_val = max(0.0, min(1.0, reward_val))\n",
        "    except:\n",
        "        reward_val = 0.0\n",
        "\n",
        "    return reward_val\n",
        "\n",
        "def q_learning_llm_with_memory(env, num_episodes=5000, alpha=0.5, gamma=0.95,\n",
        "                              initial_epsilon=1.0, min_epsilon=0.01, epsilon_decay=0.995,\n",
        "                              memory=None):\n",
        "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "    epsilon = initial_epsilon\n",
        "    rewards_per_episode = []\n",
        "    env.reset()\n",
        "    grid_map = env.render()\n",
        "\n",
        "    for i in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            if np.random.uniform(0, 1) < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(q_table[state])\n",
        "\n",
        "            next_state, _, done, truncated, info = env.step(action)\n",
        "\n",
        "            # Get reward using LLM with memory context\n",
        "            reward = get_language_reward(state, action, next_state, grid_map, memory)\n",
        "            steps += 1\n",
        "\n",
        "            # Update Q-table\n",
        "            q_table[state, action] = q_table[state, action] + alpha * (\n",
        "                reward + gamma * np.max(q_table[next_state]) - q_table[state, action]\n",
        "            )\n",
        "\n",
        "            # Add experience to memory\n",
        "            if memory:\n",
        "                memory.add_experience(state, action, next_state, reward, grid_map)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        # Normalize reward by steps\n",
        "        total_reward /= steps if steps > 0 else 1\n",
        "        rewards_per_episode.append(total_reward)\n",
        "\n",
        "        if i % 20 == 0:\n",
        "            print(f\"Episode {i+1} done\")\n",
        "\n",
        "        # Decay epsilon\n",
        "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            avg_reward = np.mean(rewards_per_episode[-100:])\n",
        "            print(f\"Episode {i+1}/{num_episodes}, Avg Reward: {avg_reward:.2f}, Epsilon: {epsilon:.2f}\")\n",
        "\n",
        "    return q_table, rewards_per_episode\n",
        "\n",
        "def visualize_agent(env, q_table, episodes=5, sleep_time=0.5, end_sleep_time=2):\n",
        "    for _ in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            clear_output(wait=True)\n",
        "            plt.imshow(env.render())\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "            sleep(sleep_time)\n",
        "\n",
        "            action = np.argmax(q_table[state])\n",
        "            state, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        plt.imshow(env.render())\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "        sleep(end_sleep_time)\n",
        "\n",
        "def print_q_table(q_table, env):\n",
        "    \"\"\"Prints the Q-table in a readable format using pandas DataFrame.\"\"\"\n",
        "    actions = ['Left', 'Down', 'Right', 'Up']\n",
        "    df = pd.DataFrame(q_table, columns=actions)\n",
        "    df.index.name = 'State'\n",
        "\n",
        "    print(\"\\n===== Q-Table =====\")\n",
        "    print(df.round(2))  # Round to 2 decimal places for readability\n",
        "    print(\"===================\\n\")\n"
      ],
      "metadata": {
        "id": "_wNfkQ89CzR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create environment\n",
        "env = gym.make(\"FrozenLake-v1\", render_mode=\"ansi\")\n",
        "num_eps = 200\n",
        "\n",
        "# Run Q-learning with memory\n",
        "q_table, rewards = q_learning_llm_with_memory(env, num_episodes=num_eps, memory=llm_memory)\n"
      ],
      "metadata": {
        "id": "7DJHkcP8EGMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the rewards\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.plot(rewards)\n",
        "plt.title('Rewards per Episode')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Average Reward')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ANCUuZwaCzBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the Q-Table\n",
        "print_q_table(q_table, env)"
      ],
      "metadata": {
        "id": "g4frNr3KEKOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up and persist memory\n",
        "vectorstore.persist()"
      ],
      "metadata": {
        "id": "xkleh4yeELf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the agent's performance\n",
        "env = gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\")\n",
        "visualize_agent(env, q_table, episodes=1, sleep_time=0.05, end_sleep_time=1)"
      ],
      "metadata": {
        "id": "r5lfXjtXEUSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.close()"
      ],
      "metadata": {
        "id": "LDumzniaEZnc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}