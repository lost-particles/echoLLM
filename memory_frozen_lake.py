# -*- coding: utf-8 -*-
"""memory_frozen_lake.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1icIEBN8yU_Sz5vmNCA1r1ZS8NtFomO3N
"""

from huggingface_hub import login
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from IPython.display import clear_output
from time import sleep
import os
from datetime import datetime
import pickle
import argparse

# Login
login_token = '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'
login(login_token)

# Load Model
model_name = "Qwen/Qwen2.5-0.5B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# Environment
env = gym.make("FrozenLake-v1", render_mode="ansi")

# Memory History Class
class TransitionHistory:
    def __init__(self, size=5):
        self.history = []
        self.size = size

    def add(self, state, action, next_state):
        self.history.append((state, action, next_state))
        if len(self.history) > self.size:
            self.history.pop(0)

    def get(self):
        return self.history.copy()

# Summary generator
def summarize_experience(episode_history):
    action_map = ['left', 'down', 'right', 'up']
    summary_prompt = (
        f"### Instruction:\n"
        f"Summarize this agent's experience in Frozen Lake. What mistakes or good moves did it make?\n"
        f"Transitions:\n" +
        "\n".join([f"{s} -> {ns} via {action_map[a]}" for s, a, ns in episode_history]) +
        "\n### Response:\n"
    )
    inputs = tokenizer(summary_prompt, return_tensors="pt").to(device)
    output = model.generate(**inputs, max_new_tokens=100)
    return tokenizer.decode(output[0], skip_special_tokens=True)

# Main reward function
def get_language_reward(state, action, next_state, grid_map, memory=None, summary_text=None):
    action_map = ['left', 'down', 'right', 'up']
    action_name = action_map[action]

    history_str = ""
    if memory:
        history_str = f"Here are the {len(memory)} most recent moves:\n" + "\n".join(
            [f"{h[0]} -> {h[2]} via {action_map[h[1]]}" for h in memory]
        ) + "\n"

    summary_str = ""
    if summary_text:
        summary_str = f"\nAgent memory: {summary_text}\n"
    
    prompt = (
        f"### Instruction:\n"
        f"You are evaluating a move made by an agent in the Frozen Lake game.\n"
        f"The lake is a 4x4 grid with 16 states (0 to 15), where the agent starts at state 0 and must reach the goal at state 15.\n"
        f"There are holes that will end the game if the agent falls in, and loops or unnecessary steps should be avoided.\n\n"
        f"The layout of the grid is: {grid_map}.\n"
        f"{history_str}"
        f"Current move: the agent moved from state {state} to state {next_state} by going {action_name}.\n"
        f"{summary_str}"
        f"How good was this move on a scale from 0 (very bad) to 1 (excellent)?\n"
        f"Respond with a single decimal number only.\n"
        f"### Response:\n"
    )


    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    outputs = model.generate(**inputs, max_new_tokens=50)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    try:
        reward_str = response.split("### Response:")[-1].strip()
        reward_val = float(reward_str.split()[0])
        reward_val = max(0.0, min(1.0, reward_val))
    except:
        reward_val = 0.0

    return reward_val

# Q-learning loop
def q_learning_llm(env, num_episodes=5000, save_every=100, memory_type="none", alpha=0.5, gamma=0.95,
                   initial_epsilon=1.0, min_epsilon=0.01, epsilon_decay=0.995):

    q_table = np.zeros([env.observation_space.n, env.action_space.n])
    epsilon = initial_epsilon
    rewards_per_episode = []
    env.reset()
    grid_map = env.render()

    model_root = "memory-models"
    os.makedirs(model_root, exist_ok=True)
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    run_dir = os.path.join(model_root, timestamp)
    os.makedirs(run_dir, exist_ok=True)

    memory = TransitionHistory(size=5)
    summary_memory = ""
    full_episode_history = []

    for i in range(num_episodes):
        state, _ = env.reset()
        done = False
        total_reward = 0
        steps = 0
        memory.history.clear()
        full_episode_history.clear()

        while not done:
            if np.random.uniform(0, 1) < epsilon:
                action = env.action_space.sample()
            else:
                action = np.argmax(q_table[state])

            next_state, _, done, truncated, info = env.step(action)

            # Update memory
            memory.add(state, action, next_state)
            full_episode_history.append((state, action, next_state))

            if memory_type == "short":
                reward = get_language_reward(state, action, next_state, grid_map, memory=memory.get())
            elif memory_type == "summary":
                reward = get_language_reward(state, action, next_state, grid_map, summary_text=summary_memory)
            else:
                reward = get_language_reward(state, action, next_state, grid_map)

            q_table[state, action] += alpha * (
                reward + gamma * np.max(q_table[next_state]) - q_table[state, action]
            )

            state = next_state
            total_reward += reward
            steps += 1

        total_reward /= steps
        rewards_per_episode.append(total_reward)

        # Update summary memory every 100 episodes
        if memory_type == "summary" and (i + 1) % 100 == 0:
            summary_memory = summarize_experience(full_episode_history)

        if (i + 1) % 20 == 0:
            print(f"Episode {i+1} done")

        epsilon = max(min_epsilon, epsilon * epsilon_decay)

        if (i + 1) % save_every == 0:
            avg_reward = np.mean(rewards_per_episode[-save_every:])
            print(f"Episode {i+1}/{num_episodes}, Avg Reward: {avg_reward:.2f}, Epsilon: {epsilon:.2f}")
            save_path = os.path.join(run_dir, f"q_table_ep{i+1}.pkl")
            with open(save_path, "wb") as f:
                pickle.dump(q_table, f)

    return q_table, rewards_per_episode

# Q-Table
def print_q_table(q_table, env):
    actions = ['Left', 'Down', 'Right', 'Up']
    df = pd.DataFrame(q_table, columns=actions)
    df.index.name = 'State'
    print("\n===== Q-Table =====")
    print(df.round(2))
    print("===================\n")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run Q-Learning with LLM-based rewards.")
    parser.add_argument("--num_eps", type=int, default=5000, help="Number of episodes to run.")
    parser.add_argument("--save_every", type=int, default=100, help="Frequency of saving the model.")
    parser.add_argument("--memory_type", type=str, default="none", help="Type of memory to use: none, short, summary.")
    args = parser.parse_args()

    num_eps = args.num_eps
    save_every = args.save_every
    memory_type = args.memory_type
    start = datetime.now()
    q_table, rewards = q_learning_llm(env, num_episodes=num_eps, save_every=save_every, memory_type=memory_type)
    end = datetime.now()

    print(f"Start time: {start}")
    print(f"End time: {end}")
    total_time = (end - start).total_seconds()
    hours, remainder = divmod(total_time, 3600)
    minutes, seconds = divmod(remainder, 60)
    print(f"Total time: {int(hours)} hr {int(minutes)} min {seconds} sec")
    
    # Plot the rewards
    plt.figure(figsize=(20, 10))
    plt.plot(rewards)
    plt.title(f'Rewards per Episode ({memory_type} memory)')
    plt.xlabel('Episode')
    plt.ylabel('Reward')
    plt.show()
    
    # save the plot
    plot_path = "memory-models"
    timestamps = [d for d in os.listdir(plot_path) if os.path.isdir(os.path.join(plot_path, d))]
    if timestamps:
        latest_timestamp = max(timestamps)
        plot_path = os.path.join(plot_path, latest_timestamp)
    else:
        plot_path = os.path.join(plot_path, "default_run")

    plot_path = os.path.join(plot_path, f"rewards_plot_{memory_type}.png")
    plt.savefig(plot_path)

    # Print the Q-Table
    print_q_table(q_table, env)

    """ DO NOT RUN ON OOD"""
    # env = gym.make("FrozenLake-v1", render_mode="rgb_array")
    # # Visualize the agent's performance
    # visualize_agent(env, q_table, episodes=1, sleep_time=0.5, end_sleep_time=1)

    # Clean up the environment
    env.close()