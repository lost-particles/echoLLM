{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_token = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# ==== Setup ====\n",
    "\n",
    "login(login_token)\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "env = gym.make(\"Blackjack-v1\", natural=False, sab=False)\n",
    "action_map = ['stick', 'hit']\n",
    "\n",
    "class TransitionHistory:\n",
    "    def __init__(self, size=5):\n",
    "        self.history = []\n",
    "        self.size = size\n",
    "\n",
    "    def add(self, state, action, next_state):\n",
    "        self.history.append((state, action, next_state))\n",
    "        if len(self.history) > self.size:\n",
    "            self.history.pop(0)\n",
    "\n",
    "    def get(self):\n",
    "        return self.history.copy()\n",
    "\n",
    "# ==== Prompting ====\n",
    "def prepare_static_prompt_blackjack():\n",
    "    examples = [\n",
    "        \"(13, 10, False) â†’ hit â†’ (23, 10, False) # Busted â†’ 0.0\",\n",
    "        \"(20, 10, False) â†’ stick â†’ (20, 10, False) # Safe stand â†’ 1.0\",\n",
    "        \"(12, 2, False) â†’ stick â†’ (12, 2, False) # Too passive â†’ 0.3\",\n",
    "        \"(16, 10, False) â†’ hit â†’ (18, 10, False) # Risk paid off â†’ 0.8\",\n",
    "        \"(18, 10, False) â†’ hit â†’ (24, 10, False) # Unnecessary risk â†’ 0.1\",\n",
    "        \"(19, 9, False) â†’ stick â†’ (19, 9, False) # Correct stand â†’ 0.9\",\n",
    "    ]\n",
    "    prompt = (\n",
    "        \"### Instruction:\\n\"\n",
    "        \"Evaluate Blackjack moves made by an agent. Return ONLY a score from 0.0 to 1.0.\\n\"\n",
    "        \"Format: (player_sum, dealer_card, usable_ace) â†’ action â†’ next_state\\n\\n\"\n",
    "        \"Examples:\\n\" + \"\\n\".join(examples) +\n",
    "        \"\\n\\n### Now evaluate:\\n\"\n",
    "    )\n",
    "    return tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "def get_language_reward(state, action, next_state, static_prompt_ids, memory=None, log=False):\n",
    "    action_name = action_map[action]\n",
    "    history_str = \"\"\n",
    "    if memory:\n",
    "        history_str = \"Recent:\\n\" + \"\\n\".join(\n",
    "            f\"{s} â†’ {action_map[a]} â†’ {ns}\" for s, a, ns in memory\n",
    "        ) + \"\\n\"\n",
    "\n",
    "    prompt = (\n",
    "        history_str +\n",
    "        f\"{state} â†’ {action_name} â†’ {next_state}\\n\"\n",
    "        \"Score (0.0 to 1.0): Respond ONLY with a single decimal number.\\n### Response:\\n\"\n",
    "    )\n",
    "    prompt_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "    full_input_ids = torch.cat([static_prompt_ids, prompt_ids], dim=-1)\n",
    "\n",
    "    outputs = model.generate(full_input_ids, max_new_tokens=10)\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    if log:\n",
    "        print(\"==== PROMPT ====\\n\", tokenizer.decode(full_input_ids[0], skip_special_tokens=True))\n",
    "        print(\"==== RESPONSE ====\\n\", decoded.strip())\n",
    "\n",
    "    for line in reversed(decoded.strip().splitlines()):\n",
    "        try:\n",
    "            score = float(line.strip())\n",
    "            return np.clip(score, 0.0, 1.0)\n",
    "        except:\n",
    "            continue\n",
    "    return 0.0\n",
    "\n",
    "def summarize_experience(transitions):\n",
    "    text = \"Transitions:\\n\" + \"\\n\".join(\n",
    "        f\"{s} â†’ {ns} via {action_map[a]}\" for s, a, ns in transitions\n",
    "    )\n",
    "    prompt = (\n",
    "        \"### Instruction:\\nSummarize the agentâ€™s Blackjack strategy.\\n\" +\n",
    "        text + \"\\n### Summary:\\n\"\n",
    "    )\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**input_ids, max_new_tokens=100)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# ==== Training ====\n",
    "def q_learning_llm(env, num_episodes=500, memory_type=\"short\"):\n",
    "    obs_space = env.observation_space\n",
    "    nS0, nS1, nS2 = obs_space[0].n, obs_space[1].n, obs_space[2].n\n",
    "    nA = env.action_space.n\n",
    "    q_table = np.zeros((nS0, nS1, nS2, nA))\n",
    "\n",
    "    epsilon = 1.0\n",
    "    min_epsilon = 0.01\n",
    "    epsilon_decay = 0.995\n",
    "    alpha = 0.5\n",
    "    gamma = 0.95\n",
    "\n",
    "    rewards_per_episode = []\n",
    "    win_draw_loss_log = []\n",
    "    results = {\"win\": 0, \"draw\": 0, \"loss\": 0}\n",
    "\n",
    "    memory = TransitionHistory(size=3)\n",
    "    summary_text = \"\"\n",
    "    static_prompt_ids = prepare_static_prompt_blackjack()\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    run_dir = os.path.join(\"models\", f\"blackjack_llm_{timestamp}\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_llm_reward = 0\n",
    "        memory.history.clear()\n",
    "        episode_transitions = []\n",
    "\n",
    "        while not done:\n",
    "            s0, s1, s2 = state\n",
    "            action = np.random.choice(nA) if np.random.rand() < epsilon else np.argmax(q_table[s0, s1, s2])\n",
    "            next_state, env_reward, done, _, _ = env.step(action)\n",
    "\n",
    "            memory.add(state, action, next_state)\n",
    "            episode_transitions.append((state, action, next_state))\n",
    "\n",
    "            if memory_type == \"short\":\n",
    "                reward = get_language_reward(state, action, next_state, static_prompt_ids, memory.get(), log=False)\n",
    "            elif memory_type == \"summary\":\n",
    "                reward = get_language_reward(state, action, next_state, static_prompt_ids, memory=None, log=False)\n",
    "            else:\n",
    "                reward = get_language_reward(state, action, next_state, static_prompt_ids, memory=None, log=False)\n",
    "\n",
    "            ns0, ns1, ns2 = next_state\n",
    "            q_table[s0, s1, s2, action] += alpha * (reward + gamma * np.max(q_table[ns0, ns1, ns2]) - q_table[s0, s1, s2, action])\n",
    "            state = next_state\n",
    "            total_llm_reward += reward\n",
    "\n",
    "        if memory_type == \"summary\" and (ep + 1) % 100 == 0:\n",
    "            summary_text = summarize_experience(episode_transitions)\n",
    "\n",
    "        if env_reward == 1:\n",
    "            results[\"win\"] += 1\n",
    "            outcome = \"win\"\n",
    "        elif env_reward == 0:\n",
    "            results[\"draw\"] += 1\n",
    "            outcome = \"draw\"\n",
    "        else:\n",
    "            results[\"loss\"] += 1\n",
    "            outcome = \"loss\"\n",
    "\n",
    "        rewards_per_episode.append(total_llm_reward)\n",
    "        win_draw_loss_log.append(outcome)\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "        if (ep + 1) % 100 == 0:\n",
    "            avg = np.mean(rewards_per_episode[-100:])\n",
    "            print(f\"[{ep+1}/{num_episodes}] Avg reward: {avg:.3f} | Epsilon: {epsilon:.3f}\")\n",
    "            print(f\"â†’ Wins: {results['win']} | Draws: {results['draw']} | Losses: {results['loss']}\")\n",
    "            with open(os.path.join(run_dir, f\"q_table_ep{ep+1}.pkl\"), \"wb\") as f:\n",
    "                pickle.dump(q_table, f)\n",
    "\n",
    "    with open(os.path.join(run_dir, \"final_q_table.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(q_table, f)\n",
    "    with open(os.path.join(run_dir, \"rewards.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(rewards_per_episode, f)\n",
    "    with open(os.path.join(run_dir, \"results_log.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(win_draw_loss_log, f)\n",
    "\n",
    "    print(\"\\nâœ… Training Complete.\")\n",
    "    print(f\"Final â†’ Wins: {results['win']} | Draws: {results['draw']} | Losses: {results['loss']}\")\n",
    "\n",
    "    return q_table, rewards_per_episode, run_dir\n",
    "\n",
    "# ==== Run Training ====\n",
    "q_table, rewards, save_dir = q_learning_llm(env, num_episodes=500, memory_type=\"short\")\n",
    "\n",
    "# ==== Reward Plot ====\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(rewards)\n",
    "plt.title(\"LLM Rewards per Episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total LLM Reward\")\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join(save_dir, \"rewards_plot.png\"))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(rewards, bins=20)\n",
    "plt.title(\"LLM Reward Distribution\")\n",
    "plt.xlabel(\"Reward\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join(save_dir, \"reward_histogram.png\"))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_blackjack_games(q_table_path, num_episodes=10, show_llm_scores=True, max_steps=20):\n",
    "    with open(q_table_path, \"rb\") as f:\n",
    "        q_table = pickle.load(f)\n",
    "\n",
    "    static_ids = prepare_static_prompt_blackjack()\n",
    "\n",
    "    results = {\"win\": 0, \"draw\": 0, \"loss\": 0}\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        memory = TransitionHistory(3)\n",
    "        steps = 0\n",
    "\n",
    "        print(f\"\\nðŸŽ² Episode {ep+1}/{num_episodes}\")\n",
    "\n",
    "        while not done and steps < max_steps:\n",
    "            s0, s1, s2 = state\n",
    "            action = np.argmax(q_table[s0, s1, s2])\n",
    "            next_state, env_reward, done, _, info = env.step(action)\n",
    "            action_name = action_map[action]\n",
    "\n",
    "            if show_llm_scores:\n",
    "                score = get_language_reward(state, action, next_state, static_ids, memory.get(), log=True)\n",
    "                print(f\"{steps+1:02d}. {state} â†’ {action_name.upper()} â†’ {next_state} | LLM score: {score:.2f}\")\n",
    "            else:\n",
    "                print(f\"{steps+1:02d}. {state} â†’ {action_name.upper()} â†’ {next_state}\")\n",
    "\n",
    "            memory.add(state, action, next_state)\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "\n",
    "        # Determine result\n",
    "        if env_reward == 1:\n",
    "            print(\"âœ… Agent WON the game!\")\n",
    "            results[\"win\"] += 1\n",
    "        elif env_reward == 0:\n",
    "            print(\"âž– Agent DREW the game.\")\n",
    "            results[\"draw\"] += 1\n",
    "        else:\n",
    "            print(\"âŒ Agent LOST the game.\")\n",
    "            results[\"loss\"] += 1\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\nðŸ“Š Summary after\", num_episodes, \"games:\")\n",
    "    print(f\"âœ… Wins:  {results['win']}\")\n",
    "    print(f\"âž– Draws: {results['draw']}\")\n",
    "    print(f\"âŒ Losses:{results['loss']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_blackjack_games(\"/users/dgada/test/re-ai/models/blackjack_llm_2025-05-05_18-30-47/final_q_table.pkl\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
