# -*- coding: utf-8 -*-
"""ReAI-langchain-llm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gf0jqDgRbyZzc_lclutby4kXgbciBDCU
"""

!pip install gymnasium langchain chromadb --quiet

import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from IPython.display import clear_output
from time import sleep
import os
from huggingface_hub import login

# LangChain imports for memory
from langchain.memory import ConversationBufferMemory
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
from langchain.text_splitter import CharacterTextSplitter

# Log in to Hugging Face Hub
login_token = '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'
login(login_token)

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# model options:
# model_name = "meta-llama/Llama-3.2-3B-Instruct"
# model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
model_name = "Qwen/Qwen2.5-0.5B-Instruct"
# model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"

model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Check if GPU is available
device = torch.device("cpu")
if torch.cuda.is_available():
    device = torch.device("cuda")
    print("GPU available. Using device:", device)

model.to(device)

# Setup LangChain Memory
# 1. Vector store for storing past experiences
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = Chroma(embedding_function=embeddings, persist_directory="./chroma_db")

# 2. Conversation buffer for recent interactions
conversation_memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

class LLMMemory:
    def __init__(self, vector_store, conversation_memory, max_conversation_length=10):
        self.vector_store = vector_store
        self.conversation_memory = conversation_memory
        self.max_conversation_length = max_conversation_length
        self.experience_count = 0
        self.text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)

    def add_experience(self, state, action, next_state, reward, grid_map):
        """Add a new experience to memory"""
        action_map = ['left', 'down', 'right', 'up']
        action_name = action_map[action]

        experience = (
            f"Experience #{self.experience_count}: "
            f"When in state {state}, taking action {action_name} "
            f"led to state {next_state} with reward {reward:.2f}."
        )

        # Add to conversation memory
        self.conversation_memory.chat_memory.add_user_message(experience)

        # Periodically move older experiences to vector store
        if len(self.conversation_memory.chat_memory.messages) > self.max_conversation_length:
            old_messages = self.conversation_memory.chat_memory.messages[:-self.max_conversation_length]
            self.conversation_memory.chat_memory.messages = self.conversation_memory.chat_memory.messages[-self.max_conversation_length:]

            # Add old messages to vector store
            for msg in old_messages:
                docs = self.text_splitter.create_documents([msg.content])
                self.vector_store.add_documents(docs)

        # Also add current experience to vector store
        docs = self.text_splitter.create_documents([experience])
        self.vector_store.add_documents(docs)

        self.experience_count += 1

    def retrieve_similar_experiences(self, state, action, top_k=3):
        """Retrieve similar past experiences"""
        action_map = ['left', 'down', 'right', 'up']
        action_name = action_map[action]

        query = f"state {state} action {action_name}"
        docs = self.vector_store.similarity_search(query, k=top_k)
        return [doc.page_content for doc in docs]

    def get_recent_history(self):
        """Get recent conversation history"""
        return self.conversation_memory.chat_memory.messages

# Initialize memory
llm_memory = LLMMemory(vectorstore, conversation_memory)

def get_language_reward(state, action, next_state, grid_map, memory=None):
    action_map = ['left', 'down', 'right', 'up']
    action_name = action_map[action]

    # Get similar experiences if memory is available
    similar_experiences = ""
    if memory:
        experiences = memory.retrieve_similar_experiences(state, action, top_k=3)
        similar_experiences = "Past similar experiences:\n" + "\n".join(experiences) if experiences else ""

    prompt = (
        f"### Instruction:\n"
        f"You are evaluating a move made by an agent in the Frozen Lake game.\n"
        f"The lake is a 4x4 grid with 16 states (0 to 15), where the agent starts at state 0 and must reach the goal at state 15.\n"
        f"There are holes that will end the game if the agent falls in, and loops or unnecessary steps should be avoided.\n\n"
        f"The key is: S=Start, F=Frozen, H=Hole, G=Goal.\n"
        f"The layout of the grid is: {grid_map}.\n"
        f"The agent moved from state {state} to state {next_state} by going {action_name}.\n"
        f"How good was this move on a scale from 0 (very bad) to 1 (excellent)?\n"
        f"Respond with a single decimal number only.\n"
        f"### Response:\n"
    )

    inputs = tokenizer(prompt, return_tensors="pt")
    inputs = inputs.to(device)

    outputs = model.generate(**inputs, max_new_tokens=10)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    try:
        reward_str = response.split("### Response:")[-1].strip()
        reward_val = float(reward_str.split()[0])
        reward_val = max(0.0, min(1.0, reward_val))
    except:
        reward_val = 0.0

    return reward_val

def q_learning_llm_with_memory(env, num_episodes=5000, alpha=0.5, gamma=0.95,
                              initial_epsilon=1.0, min_epsilon=0.01, epsilon_decay=0.995,
                              memory=None):
    q_table = np.zeros([env.observation_space.n, env.action_space.n])
    epsilon = initial_epsilon
    rewards_per_episode = []
    env.reset()
    grid_map = env.render()

    for i in range(num_episodes):
        state, _ = env.reset()
        done = False
        total_reward = 0
        steps = 0

        while not done:
            if np.random.uniform(0, 1) < epsilon:
                action = env.action_space.sample()
            else:
                action = np.argmax(q_table[state])

            next_state, _, done, truncated, info = env.step(action)

            # Get reward using LLM with memory context
            reward = get_language_reward(state, action, next_state, grid_map, memory)
            steps += 1

            # Update Q-table
            q_table[state, action] = q_table[state, action] + alpha * (
                reward + gamma * np.max(q_table[next_state]) - q_table[state, action]
            )

            # Add experience to memory
            if memory:
                memory.add_experience(state, action, next_state, reward, grid_map)

            state = next_state
            total_reward += reward

        # Normalize reward by steps
        total_reward /= steps if steps > 0 else 1
        rewards_per_episode.append(total_reward)

        if i % 20 == 0:
            print(f"Episode {i+1} done")

        # Decay epsilon
        epsilon = max(min_epsilon, epsilon * epsilon_decay)

        if (i + 1) % 100 == 0:
            avg_reward = np.mean(rewards_per_episode[-100:])
            print(f"Episode {i+1}/{num_episodes}, Avg Reward: {avg_reward:.2f}, Epsilon: {epsilon:.2f}")

    return q_table, rewards_per_episode

def visualize_agent(env, q_table, episodes=5, sleep_time=0.5, end_sleep_time=2):
    for _ in range(episodes):
        state, _ = env.reset()
        done = False

        while not done:
            clear_output(wait=True)
            plt.imshow(env.render())
            plt.axis('off')
            plt.show()
            sleep(sleep_time)

            action = np.argmax(q_table[state])
            state, reward, done, truncated, info = env.step(action)

        clear_output(wait=True)
        plt.imshow(env.render())
        plt.axis('off')
        plt.show()
        sleep(end_sleep_time)

def print_q_table(q_table, env):
    """Prints the Q-table in a readable format using pandas DataFrame."""
    actions = ['Left', 'Down', 'Right', 'Up']
    df = pd.DataFrame(q_table, columns=actions)
    df.index.name = 'State'

    print("\n===== Q-Table =====")
    print(df.round(2))  # Round to 2 decimal places for readability
    print("===================\n")

# Create environment
env = gym.make("FrozenLake-v1", render_mode="ansi")
num_eps = 200

# Run Q-learning with memory
q_table, rewards = q_learning_llm_with_memory(env, num_episodes=num_eps, memory=llm_memory)

# Plot the rewards
plt.figure(figsize=(20, 10))
plt.plot(rewards)
plt.title('Rewards per Episode')
plt.xlabel('Episode')
plt.ylabel('Average Reward')
plt.show()

# Print the Q-Table
print_q_table(q_table, env)

# Clean up and persist memory
vectorstore.persist()

# Visualize the agent's performance
env = gym.make("FrozenLake-v1", render_mode="rgb_array")
visualize_agent(env, q_table, episodes=1, sleep_time=0.05, end_sleep_time=1)

env.close()